<!DOCTYPE html>
<html>
<head>
  <style>
    body {
      background-color: black;
      color: white;
      font-family: Arial, sans-serif;
    }
    a {
      color: white;
    }
    a:hover {
      color: #ccc;
    }
  </style>
</head>
<t>NO LONGER VALID - ARCHIVED COPY ONLY</t>
<body>
<h1>Common concerns</h1>

<h2>I don’t have the budget for an AI risk assessment. How should I pitch this internally?</h2>
    <p>Ask your engineering, product, and finance colleagues the following questions:
    <ul>
        <li>Do we have budget to lose deals to competitors because we cannot explain our AI security posture to prospects (while you are also blasting out blogs and social posts about how we use AI throughout the company!)?</li>
        <li>Do we have budget to address lost competitive advantage from employees unintentionally training SaaS generative AI models on our intellectual property like <a href='https://blog.stackaware.com/p/amazon-data-leakage-unintended-training'>Amazon</a> did, losing nearly $1.5 million?</li>
        <li>Do we have budget to jump through our a** before our next audit to document how we are meeting compliance requirements while using AI?</li>
    </ul>
    </p>
    <h2>Shouldn’t we wait until we have finished our AI project(s) before engaging you?</h2>
    <p>If you would prefer engineering and product teams to start deploying AI-powered products rapidly, without worrying about cybersecurity or privacy, then this makes sense. But what are you going to do after they have already:
    <ul>
        <li>Been using ChatGPT every day for almost two years?</li>
        <li>Integrated open source AI libraries into their code?</li>
        <li>Connected workflows to SaaS-hosted LLMs?</li>
    </ul>
    If you think you are going to be able to apply guardrails after all of this is already done, think again. Even the forward-leaning companies we have already helped had to battle some inertia because of the previous pace of AI-related development. Security teams often get left out of discussions about new product and feature development, so the time is now (frankly, yesterday) to start talking about how you are going to:
    <ul>
        <li>Meet regulatory obligations</li>
        <li>Classify and govern data</li>
        <li>Threat model</li>
        <li>Vet vendors</li>
    </ul>
    </p>
</div>

<h2>Ok, so what level of sophistication with AI does my company need to have for you to conduct a risk assessment?</h2>
<p>We can do an AI risk assessment at any stage of development or maturity. The process is designed to be flexible and can even incorporate future roadmap plans. This means that even if development or rollout is in its early stages, a risk assessment can still provide valuable insights for strategic planning and development. In fact, this is the best time to start!</p>

    <h2> Shouldn’t we hire someone to build this in-house?</h2>
    <p>Having a full-time employee run your AI GRC program might make sense under certain conditions. For example:

      <h3>You have or can acquire sufficient in-house leadership expertise.</h3>
       <p>This can certainly be true if you are a big 4 U.S. bank or equivalent. Bank of America, for example, made Ann Chang the global head of “security of AI.”</p>
        <p>For organizations with the right resources, doing it yourself might be the best way to go.</p>
        <p>If you can’t afford to dedicate at least 25% of a senior leader’s time to this task (and find someone capable of the role), though, you might consider getting external help immediately.</p>
        <h3>You can monitor the constantly-changing landscape yourself.</h3>
        <p>With the correct mix of:
            <ul>
                <li>security tooling</li>
                <li>procedures</li>
                <li>training</li>
            </ul>
            this may be possible to do on your own.</p>
        <p>You'll need to stay on top of:
            <ul>
                <li>best practices in terms of confidentiality and IP assignment</li>
                <li>evolving norms regarding data retention and training</li>
                <li>4th party AI use</li>
            </ul>
            But otherwise, it probably makes sense to leverage a specialist solely focused on these issues.</p>
        <h3>You want to focus on AI governance in addition to business use cases.</h3>
        <p>You will know the business goals of your AI projects better than anyone else, so for certain extremely niche situations, achieving them requires strict oversight of the GRC program as well.</p>
        <p>While being able to exert complete control over your program by having employees build it may sound appealing, also consider the opportunity costs:
            <ul>
                <li>negotiation with internal stakeholders with overlapping concerns.</li>
                <li>inability to leverage best practices developed elsewhere.</li>
                <li>heavy initial investment to pay down “ignorance debt.”</li>
            </ul>
        </l>
    </ul>
    </p>
<p>You can also see this video response <a href ='https://youtube.com/shorts/koDleswM3pU'>here</a></p>

<h2>Will I be reliant on you to keep my AI governance, risk, and compliance program running indefinitely?</h2>
<p>No. StackAware's goal is to</p>
  <ul>
<li>quickly get you up to speed in terms of governance
<li>move into maintenance mode right afterward
<li>eventually move you to a SaaS platform
  </ul>
<p>As we (and the entire industry) builds out best practices for AI security and risk management, white glove service is definitely appropriate for certain organizations.</p>
<p>When these standards solidify, it will be time to move into self-service mode.</p>
<p>Check out this <a href='https://youtube.com/shorts/dx6E5x2hqbg'>video response</a> as well</p>
  
<h1>Determining the right fit</h1>
<h2> Are you focused on mitigating AI-powered threats (malware, phishing, etc.)?</h2>
<p> No. StackAware is focused on enabling the secure use of AI technologies and systems, not defending against AI-powered threats.</p>

<h2> We think we have been hacked. Do you do incident response or forensics?</h2>
<p>No. StackAware is focused entirely on preventing AI-related cybersecurity incidents. We do not manage incident responses or post-breach investigations. We do, however, have referral partners whom we can recommend.</p>

<h2>Do you have a data sheet I can review and share internally?</h2>
    <p>Yes. We have ones available for both our <a href='https://assessment-datasheet.stackaware.com/'>assessment</a> and <a href='https://governance-datasheet.stackaware.com/'>governance</a> offerings.</p>

<h2>We are a software product company that provides software partially or exclusively for customers to run in their own environments (i.e. not -as-a-Service). Do you have experience working in these types of situations?</h2>
<p>Yes. We have experience at multiple companies that have offered both customer-managed (e.g., on-premises) and vendor-managed (e.g., SaaS) products and know how to handle both models.</p>

 <h2>What is required for an AI penetration test?</h2>
<p>You’ll need a functioning application the tester can interact with, either with a user (UI) or application programming interface (API). It can be in the early stages, but the tester will need to be able to authenticate and supply inputs to the application.</p>

<h1>Logistics</h1>

<h2>Do you just email a PDF at the end of engagement?</h2>
<p>No. StackAware provides a machine-readable risk register (in .csv format) that you can upload to your governance, risk, and compliance (GRC) tool. We also provide a Google Slides presentation that is executive-friendly.</p>

<h2>How do you use AI? How do you protect my data?</h2>
<p>StackAware is an AI-powered company and uses generative AI technologies like GPT for a variety of use cases. Specifically, we use retrieval-augmented generation (RAG) techniques with GPT-4 to analyze customer architectures and technological approaches for compliance implications.<p>

<p>To protect security-sensitive customer data when doing so, StackAware uses only the OpenAI API - which has more favorable data retention policies than the ChatGPT user interface and is opted out of training by default - for RAG analysis. Additionally, out of an abundance of caution, StackAware uses an automated approach to sanitize customer names and identifiers from data prior to its passage to the OpenAI API. Thus, even if there were some data breach or leakage caused by OpenAI, the specific information would not be attributable to your organization.</p>

<p>Separately, StackAware has executed a Data Processing Addendum (DPA) with OpenAI to govern the handling of any personal data processed by the service.</p>

<p>We pride ourselves on our transparency and list all known AI processing in our supply chain in our software bill of materials (<a href='https://sbom.stackaware.com/'>SBOM</a>).</p>
<p> You can also learn more about our data security practices at our <a href='https://security.stackaware.com/'>trust center</a>.</p>

<h2>Why do I need to designate a business leader who is accountable for the organization’s security? Shouldn’t that be a security person?</h2>
<p>While security might be everyone’s responsibility, only one person should be accountable for it, at the end of the day. Ensuring a leader with holistic responsibility for your business and all the risks it faces is the one who makes risk management decisions is key to having an effective and coherent security program.</p>

<h2>Why do I need to designate a single security advisor in our contract? Isn’t that you?</h2>
<p>StackAware enables your security program to succeed by applying not only technical but organizational best practices. Ensuring you have a single, full-time individual responsible for giving security advice and implementing business decisions is the best way to set you up for success. If we do our job right and allow your business to scale, eventually you won’t need our consulting services anymore. Grooming a leader to take over when that happens is the best way to prepare for this.</p>

</body>
</html>
